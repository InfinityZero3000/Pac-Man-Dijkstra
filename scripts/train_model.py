#!/usr/bin/env python3
"""
Train a simple CNN to predict the next step towards the goal given a maze and positions.
Supervised imitation on optimal paths generated by the solver.
"""
import os
import json
from glob import glob
from dataclasses import dataclass
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader


def one_hot_grid(maze, start, goal):
    h, w = len(maze), len(maze[0])
    wall = np.array(maze, dtype=np.float32)
    start_ch = np.zeros_like(wall)
    goal_ch = np.zeros_like(wall)
    sy, sx = start
    gy, gx = goal
    start_ch[sy, sx] = 1.0
    goal_ch[gy, gx] = 1.0
    return np.stack([wall, start_ch, goal_ch], axis=0)  # [3, H, W]


def direction_from_step(a, b):
    dy, dx = b[0] - a[0], b[1] - a[1]
    # 0: up, 1: down, 2: left, 3: right
    if dy == -1 and dx == 0:
        return 0
    if dy == 1 and dx == 0:
        return 1
    if dy == 0 and dx == -1:
        return 2
    if dy == 0 and dx == 1:
        return 3
    return -1


class MazeStepsDataset(Dataset):
    def __init__(self, dataset_file):
        with open(dataset_file, 'r') as f:
            data = json.load(f)
        # Expand into (state, action) steps
        samples = []
        for item in data:
            maze = item['maze_features']['maze_pattern']
            start = tuple(item['start_position'])
            goal = tuple(item['goal_position'])
            path = item['optimal_path']
            for i in range(len(path) - 1):
                a, b = tuple(path[i]), tuple(path[i+1])
                action = direction_from_step(a, b)
                if action < 0:
                    continue
                # Represent state with current position as start
                grid = one_hot_grid(maze, a, goal)
                samples.append((grid, action))
        self.samples = samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        grid, action = self.samples[idx]
        return torch.tensor(grid, dtype=torch.float32), torch.tensor(action, dtype=torch.long)


class ConvPolicy(nn.Module):
    def __init__(self, hidden=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, hidden, 3, padding=1), nn.ReLU(),
            nn.Conv2d(hidden, hidden, 3, padding=1), nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
        )
        self.head = nn.Linear(hidden, 4)

    def forward(self, x):
        z = self.net(x)
        z = z.view(z.size(0), -1)
        return self.head(z)


@dataclass
class TrainConfig:
    dataset_file: str
    epochs: int = 5
    batch_size: int = 64
    lr: float = 1e-3
    device: str = 'cpu'


def train(cfg: TrainConfig):
    ds = MazeStepsDataset(cfg.dataset_file)
    if len(ds) == 0:
        raise RuntimeError('Dataset is empty; generate training data first.')
    dl = DataLoader(ds, batch_size=cfg.batch_size, shuffle=True)
    model = ConvPolicy().to(cfg.device)
    opt = optim.Adam(model.parameters(), lr=cfg.lr)
    loss_fn = nn.CrossEntropyLoss()

    for ep in range(cfg.epochs):
        model.train()
        total, correct, total_loss = 0, 0, 0.0
        for x, y in dl:
            x, y = x.to(cfg.device), y.to(cfg.device)
            opt.zero_grad()
            logits = model(x)
            loss = loss_fn(logits, y)
            loss.backward()
            opt.step()
            total_loss += float(loss.item()) * x.size(0)
            pred = logits.argmax(dim=1)
            correct += int((pred == y).sum().item())
            total += x.size(0)
        print(f"epoch {ep+1}/{cfg.epochs} - loss {total_loss/total:.4f} - acc {correct/total:.3f}")

    os.makedirs('models', exist_ok=True)
    out = 'models/conv_policy.pt'
    torch.save(model.state_dict(), out)
    print('Saved model to', out)


if __name__ == '__main__':
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument('--dataset', type=str, required=True,
                   help='Path to training_data/training_sets/training_dataset_*.json')
    p.add_argument('--epochs', type=int, default=5)
    p.add_argument('--batch-size', type=int, default=64)
    p.add_argument('--lr', type=float, default=1e-3)
    args = p.parse_args()
    cfg = TrainConfig(dataset_file=args.dataset, epochs=args.epochs, batch_size=args.batch_size, lr=args.lr)
    train(cfg)
